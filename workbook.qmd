---
title: "Workshop Workbook - Sentiment Analysis in R"
format: html
editor: visual
---

# Setup

### *Improve qmd rendering*

```{r setup, include=FALSE}
# Setting global chunk options
knitr::opts_chunk$set(
  echo = FALSE,      # do not show code
  message = FALSE,   # suppress messages
  warning = FALSE    # suppress warnings
)
```

### *Install & Load Required Packages*

To complete our workflow we will need a dozen packages. **Copy the code to install it via your console** for on-time setup:

```{r packages_install, eval=FALSE}
install.packages(c(
  "emoji",
  "ggplot2",
  "lexicon",
  "sentimentr",
  "stopwords",
  "stringi",
  "syuzhet",
  "textclean",
  "textstem",
  "tidyr",
  "tidytext",
  "tidyverse"
))
```

```{r pakages_load, warning=FALSE}
library(dplyr)       # data wrangling
library(emoji)       # emoji conversion
library(ggplot2)     # plots
library(lexicon)     # used by sentimentr
library(readr)       # read/write CSV
library(sentimentr)  # polarity/sentiment analysis
library(stopwords)   # stopword removal
library(stringi)     # string operations (emoji replacement)
library(stringr)     # string manipulation
library(syuzhet)     # NRC emotion lexicon
library(textclean)   # expand contractions
library(textstem)    # lemmatization
library(tidyr)       # data reshaping
library(tidytext)    # tokenization
library(tidyverse)   # data manipulation and visualization
```

# PART I. Normalizing the Data

### *Load & Inspect the Data*

```{r inspect_comments}
comments <- read_csv("comments.csv")

head(comments$text)
```

### *Applying Normalization*

```{r normalizing}
# Normalize apostrophes
comments$text <- gsub("[’‘ʼ`]", "'", comments$text)

# Expand contractions
comments <- comments %>%
  mutate(text_expand = replace_contraction(text))

# Convert to lowercase
comments <- comments %>%
  mutate(text_lower = tolower(text_expand))

# Remove URLs
comments <- comments %>%
  mutate(text_nourl = str_replace_all(text_lower, "http[s]?://[^\\s,]+|www\\.[^\\s,]+", ""))

# Remove mentions (@username)
comments <- comments %>%
  mutate(text_nomention = str_replace_all(text_nourl, "@[A-Za-z0-9_]+", ""),
         text_nomention = str_squish(text_nomention))

# Remove punctuation & numbers
comments <- comments %>%
  mutate(text_cleaned = text_nomention %>%
           str_replace_all("[[:punct:]“”‘’–—…|]", "") %>%
           str_replace_all("[[:digit:]]", "") %>%
           str_replace_all("(.)\\1{2,}", "\\1") %>%  # collapse rep. letters eg. "Amaaazing"
           str_squish()
  )

# Convert emojis to text
emoji_dict <- emo::jis[, c("emoji", "name")]
emoji_dict$name <- paste0("[", emoji_dict$name, "]")

replace_emojis <- function(text, emoji_dict) {
  stri_replace_all_fixed(
    str = text, 
    pattern = emoji_dict$emoji, 
    replacement = emoji_dict$name, 
    vectorize_all = FALSE
  )
}

comments <- comments %>%
  mutate(text_noemoji = replace_emojis(text_cleaned, emoji_dict),
         text = str_replace_all(text_noemoji, "[^[:alpha:][:space:]]", ""),
         text = str_squish(text))

# Save normalized data
write_csv(comments %>% select(id, text), "normalized.csv")
```

# PART II. Text Pre-Processing

### *Load & Inspect Normalized Data*

```{r inspect_normalized}
normalized <- read_csv("normalized.csv")

head(normalized$text)
```

### *Implementing Tokenization, Stop Words Removal and Lemmatization*

```{r}
# Force 'id' to be read as character to preserve values like "s1_0001"
data <- read_csv("normalized.csv",
                 col_types = cols(
                   id = col_character(),
                   text = col_character()
                 ))

# Check first few rows
head(data)

# Tokenize text
tokens <- data %>%
  unnest_tokens(word, text) # creates one row per word

# Inspect tokens
head(tokens)

# Remove stop words
tokens_nostop <- tokens %>%
  filter(!word %in% stopwords("en"))

# Lemmatize words their base form
tokens_nostop <- tokens_nostop %>%
  mutate(word_lemmatized = lemmatize_words(word))

# Remove any remaining single-character words
tokens_nostop <- tokens_nostop %>%
  filter(str_length(word_lemmatized) > 1)

# Reconstruct cleaned text into sentenced per ID
preprocessed <- tokens_nostop %>%
  group_by(id) %>%
  summarise(text_ready = paste(word_lemmatized, collapse = " ")) %>%
  ungroup()

# Save to CSV with two columns: 'id' and 'text_ready'
write.csv(preprocessed, "preprocessed.csv")
```

# PART III. Sentiment Analysis

### *Load & Inspect the Pre-processed Data (text_ready)*

```{r inspecting_data}
preprocessed <- read_csv("preprocessed.csv")

head(preprocessed$text_ready)
```

### *Polarity with Sentiment R*

```{r polarity}

# Compute sentiment per row/case
sentiment_scores <- sentiment_by(data$text)

# Bind results back by row index
data <- data %>%
  mutate(score = sentiment_scores$ave_sentiment,
         sentiment_label = case_when(
           score > 0.1  ~ "positive",
           score < -0.1 ~ "negative",
           TRUE         ~ "neutral"
         ))

head(data)

# Visualize

ggplot(data, aes(x = score)) +
  geom_histogram(binwidth = 0.1, fill = "skyblue", color = "white") +
  theme_minimal() +
  labs(title = "Sentiment Score Distribution", x = "Average Sentiment", y = "Count")

table(data$sentiment_label)

# Check for extreme cases
head(data[order(data$score), ], 10)   # Most negative
head(data[order(-data$score), ], 10)  # Most positive

# Plotting Scores
ggplot(data, aes(x = seq_along(score), y = score)) +
  geom_line(color = "steelblue") +
  geom_smooth(se = FALSE, color = "orange") +
  theme_minimal() +
  labs(title = "Sentiment Trajectory by Entry", x = "Entry Order", y = "Average Sentiment")

# Extract season info (s1, s2) into a new column
data <- data %>%
  mutate(season = str_extract(id, "s\\d+"))

# Histogram comparison by season
ggplot(data, aes(x = score, fill = season)) +
  geom_histogram(binwidth = 0.1, position = "dodge", color = "white") +
  theme_minimal() +
  labs(title = "Sentiment Score Distribution by Season", 
       x = "Average Sentiment", y = "Count") +
  scale_fill_brewer(palette = "Set1")

# Sentiment trajectory by season (faceted)
ggplot(data, aes(x = seq_along(score), y = score, color = season)) +
  geom_line(alpha = 0.7) +
  geom_smooth(se = FALSE) +
  theme_minimal() +
  labs(title = "Sentiment Trajectory by Season", 
       x = "Entry Order", y = "Average Sentiment") +
  facet_wrap(~season, scales = "free_x")

# Extract season info
data <- data %>%
  mutate(season = str_extract(id, "s\\d+")) %>%
  group_by(season) %>%
  mutate(entry_order = row_number()) %>%  # order within season
  ungroup()
```

### *Fine-grained Emotions with* Syuzhet's NRC Emotion Lexicon

```{r fine-grained}
emotion_scores <- get_nrc_sentiment(preprocessed$text_ready)
data_emotions <- bind_cols(preprocessed, emotion_scores)

# Summarize overall emotions
emotion_summary <- emotion_scores %>%
  summarise(across(everything(), sum)) %>%
  pivot_longer(cols = everything(), names_to = "emotion", values_to = "count")

# Plot overall emotions
ggplot(emotion_summary, aes(x = reorder(emotion, -count), y = count, fill = emotion)) +
  geom_col(show.legend = FALSE) +
  scale_fill_brewer(palette = "Set2") +
  theme_minimal(base_size = 14) +
  labs(title = "Overall Emotion Distribution", x = "Emotion", y = "Total Count")

# Combine polarity & emotions
final_table <- data_emotions %>%
  select(id, text_analysis, score, sentiment_label, anger:trust) %>%
  pivot_longer(cols = anger:trust, names_to = "emotion", values_to = "emotion_score")

# Save results
write_csv(final_table, "sentiment_emotion_results.csv")

# Emotion co-occurrence
emotion_binary <- (select(data_emotions, anger:trust) > 0) * 1
co_matrix <- t(as.matrix(emotion_binary)) %*% as.matrix(emotion_binary)
co_df <- as.data.frame(as.table(co_matrix))
colnames(co_df) <- c("Emotion1", "Emotion2", "Cooccurrence")

# Co-occurrence heatmap
ggplot(co_df %>% filter(Emotion1 != Emotion2), aes(x = Emotion1, y = Emotion2, fill = Cooccurrence)) +
  geom_tile(color = "white") +
  scale_fill_gradient(low = "white", high = "darkred") +
  theme_minimal(base_size = 14) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title = "Emotion Co-occurrence Heatmap", x = "Emotion", y = "Emotion", fill = "Co-occurrence")
```
